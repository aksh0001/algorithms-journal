{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_DQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksh0001/Algorithm-Journal/blob/master/RL_DQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwxfZCFCZYSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch as T\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XNSxZt_aga5",
        "colab_type": "text"
      },
      "source": [
        "Create DQN (conv2d->conv2d->conv2d->fc->fc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLrbixRtafQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, LR):\n",
        "    super(DQN, self).__init__()\n",
        "    # use conv to reduce state space\n",
        "    self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)  # 1 input channel (color doesn't matter - save computation)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "    \n",
        "    self.fc1 = nn.Linear(128*19*8, 512)\n",
        "    self.fc2 = nn.Linear(512, 6)  # 6 actions (L,R,shoot static, shoot while moving left, shoot while moving right,skip)\n",
        "    \n",
        "    self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "    \n",
        "  def forward(self, observation):\n",
        "    obs = T.Tensor(observation).to(self.device)  # convert sequence of frames to Tensor\n",
        "    obs = obs.view(-1, 1, 185, 95)  # reshape for conv layer\n",
        "    obs = F.relu(self.conv1(obs))\n",
        "    obs = F.relu(self.conv2(obs))\n",
        "    obs = F.relu(self.conv3(obs))\n",
        "    \n",
        "    # flatten convolved images; then feed into fc\n",
        "    obs = obs.view(-1, 128*19*8)\n",
        "    obs = F.relu(self.fc1(obs))\n",
        "    \n",
        "    actions = self.fc2(obs)\n",
        "    \n",
        "    return actions  # this will be a matrix: k x 6 where k=num imgs passed in"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOX-BXsGhKyS",
        "colab_type": "text"
      },
      "source": [
        "Creating our Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o13wTvVyhB1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, gamma, eps, LR, max_mem_size, eps_end=0.5, replace=10000, action_space=[0,1,2,3,4,5]):\n",
        "    self.gamma = gamma\n",
        "    self.eps = eps\n",
        "    self.eps_end = eps_end\n",
        "    self.action_space = action_space\n",
        "    self.max_mem_size = max_mem_size\n",
        "    self.steps = 0\n",
        "    self.learn_step_counter = 0  # for target network replacement\n",
        "    self.memory = []  # use as a list\n",
        "    self.mem_counter = 0\n",
        "    self.replace_target_counter = replace\n",
        "    self.Q_eval = DQN(LR)  # agent's estimate of the current set of states\n",
        "    self.Q_next = DQN(LR)  # agent's estimate of the successor set of states\n",
        "    \n",
        "  def store_transition(self, state, action, reward, resulting_state):\n",
        "    if self.mem_counter < self.max_mem_size:\n",
        "      self.memory.append([state, action, reward, resulting_state])\n",
        "    else:\n",
        "      self.memory[self.mem_counter%self.max_mem_size] = [state, action, reward, resulting_state]\n",
        "    \n",
        "    self.mem_counter += 1\n",
        "    \n",
        "  def choose_action(self, observation):\n",
        "    # we pass in a sequence of observations\n",
        "    rand = np.random.random()\n",
        "    actions = self.Q_eval.forward(observation)\n",
        "    if rand < 1 - self.eps:\n",
        "      action = T.argmax(actions[1]).item()\n",
        "    else:\n",
        "      action = np.random.choice(self.action_space)\n",
        "    \n",
        "    self.steps += 1\n",
        "    return action\n",
        "  \n",
        "  \n",
        "  def learn(self, batch_size):\n",
        "    self.Q_eval.optimizer.zero_grad()  # batch learning, zero grad\n",
        "    if self.replace_target_counter is not None and self.learn_step_counter%self.replace_target_counter == 0:\n",
        "      self.Q_next.load_state_dict(self.Q_eval.state.dict())\n",
        "      \n",
        "    if self.mem_counter + batch_size < self.max_mem_size:\n",
        "      mem_start = int(np.random.choice(range(self.mem_counter)))\n",
        "    else:\n",
        "      mem_start = int(np.random.choice(range(self.max_mem_size-batch_size-1)))\n",
        "    \n",
        "    mini_batch = self.memory[mem_start:mem_start+batch_size]\n",
        "    memory = np.array(mini_batch)\n",
        "    \n",
        "    Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)\n",
        "    Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)\n",
        "    \n",
        "    maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)\n",
        "    rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
        "    Qtarget = Qpred\n",
        "    Qtarget[:, maxA] = rewards + self.gamma*T.max(Qnext[1])\n",
        "    \n",
        "    if self.steps > 500:\n",
        "      if self.eps - 1e-4 > self.eps_end:\n",
        "        self.eps -= 1e-4  # converge epsilon\n",
        "      else:\n",
        "        self.eps = self.eps_end\n",
        "    \n",
        "    loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
        "    loss.backward()\n",
        "    self.Q_eval.optimizer.step()\n",
        "    self.learn_step_counter += 1\n",
        "   \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIc05Pq8nu58",
        "colab_type": "text"
      },
      "source": [
        "Driver loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZAkarUEnxH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL12bgbMoRkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9467831c-0a99-4b77-aa4d-b7d8e1207cdd"
      },
      "source": [
        "environment = gym.make('SpaceInvaders-v0')\n",
        "agent = Agent(gamma=0.95, eps=1.0,LR=0.03,max_mem_size=5000,replace=None)\n",
        "\n",
        "# init memory\n",
        "while agent.mem_counter < agent.max_mem_size:\n",
        "  obs = environment.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    # 0=no action;1=fire;2=right;3=left;4=move right fire;5=move left fire\n",
        "    action = environment.action_space.sample()\n",
        "    obs_, reward, done, info = environment.step(action)\n",
        "    if done and info['ale.lives'] == 0:\n",
        "      reward = -100\n",
        "    \n",
        "    agent.store_transition(np.mean(obs[15:200, 30:125], axis=2), action, reward,\n",
        "                         np.mean(obs_[15:200, 30:125], axis=2))\n",
        "    obs = obs_\n",
        "  \n",
        "print('Done memory init')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done memory init\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-aF2wVeH-mkR",
        "colab": {}
      },
      "source": [
        "scores = []\n",
        "eps_history = []\n",
        "num_games = 50\n",
        "batch_size = 32\n",
        "\n",
        "for i in range(num_games):\n",
        "  print('game start',i+1,'eps: %.4f'%agent.eps)\n",
        "  eps_history.append(agent.eps)\n",
        "  done = False\n",
        "  obs = environment.reset()\n",
        "  frames = [np.sum(obs[15:200, 30:125], axis=2)]\n",
        "  score = 0\n",
        "  last_action = 0\n",
        "  \n",
        "  while not done:\n",
        "    if len(frames) == 3:\n",
        "      action = agent.choose_action(frames)\n",
        "      frames = []\n",
        "    else:\n",
        "      action = last_action\n",
        "     \n",
        "    obs_, reward, done, info = environment.step(action)\n",
        "    score += reward\n",
        "    frames.append(np.sum(obs_[15:200, 30:125], axis=2))\n",
        "    if done and info['ale.lives'] == 0:\n",
        "      reward = -100\n",
        "      \n",
        "    agent.store_transition(np.mean(obs[15:200, 30:125], axis=2), action, reward,\n",
        "                         np.mean(obs_[15:200, 30:125], axis=2))\n",
        "    \n",
        "    obs = obs_\n",
        "    agent.learn(batch_size)\n",
        "    last_action = action\n",
        "    # environment.render()  # see environment\n",
        "  \n",
        "  scores.append(score)\n",
        "  print('score: ', score)\n",
        "  x = [i + 1 for i in range(num_games)]\n",
        "  fname = 'test' + str(num_games) + '.png'\n",
        "  # plot x, scores, eps_history and save into file"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}